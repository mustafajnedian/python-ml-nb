{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdXvr15DBqw+hBE8Q6/1ph",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mustafajnedian/python-ml-nb/blob/main/ML_Vectorizing_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ Which One Should You Use in APP?\n",
        "\n",
        "‚úÖ Use Count Vectorizer ‚Üí If you need basic word frequency features.\n",
        "\n",
        "‚úÖ Use TF-IDF Vectorizer ‚Üí If you need importance-based word weighting.\n",
        "\n",
        "‚úÖ Use Word2Vec ‚Üí If you want semantic understanding (best for deep learning)."
      ],
      "metadata": {
        "id": "zdNN2inqUFEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå app's Text Processing Pipeline\n",
        "- Input: User searches for a recipe like ```\"Spicy chicken curry\"```\n",
        "- Processing: Convert the input and stored recipes into numerical form using `Count Vectorizer`, `TF-IDF`, or `Word2Vec`\n",
        "- Output: Find the most similar recipe from the database"
      ],
      "metadata": {
        "id": "3kjS7y6EURhU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "-r_NYDx5T9M5"
      },
      "outputs": [],
      "source": [
        "recipes = [\n",
        "    \"Spicy chicken and curry with rice and potatoes herbs and apples\",\n",
        "    \"Mild chicken stew with vegetables\",\n",
        "    \"Hot and spicy beef stir fry\",\n",
        "    \"Vegan lentil soup with coconut milk\",\n",
        "    \"Grilled salmon with garlic butter\"\n",
        "]\n",
        "\n",
        "query = \"Spicy chicken vegetables rice\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1Ô∏è‚É£ Using Count Vectorizer for Recipe Similarity\n",
        "üìå What happens?\n",
        "\n",
        "Each recipe is converted into a bag-of-words count vector\n",
        "Cosine Similarity is used to find the closest match"
      ],
      "metadata": {
        "id": "zy7t88FpUhYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Convert text to Count Vectors\n",
        "vectorizer = CountVectorizer()\n",
        "recipe_vectors = vectorizer.fit_transform(recipes)\n",
        "query_vector = vectorizer.transform([query])\n",
        "\n",
        "# Compute similarity\n",
        "similarities = cosine_similarity(query_vector, recipe_vectors)\n",
        "\n",
        "# Find the most similar recipe\n",
        "best_match_index = similarities.argmax()\n",
        "print(f\"Best Match (CountVectorizer): {recipes[best_match_index]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwjeKZuIUBh-",
        "outputId": "cf9c9490-074a-4d94-ff39-7f85d6976b40"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Match (CountVectorizer): Mild chicken stew with vegetables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2Ô∏è‚É£ Using TF-IDF for Recipe Similarity\n",
        "üìå What happens?\n",
        "\n",
        "Words like \"spicy\" and \"chicken\" are weighted based on their importance\n",
        "Helps distinguish common vs. unique words"
      ],
      "metadata": {
        "id": "17vBy2UAV6Cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Convert text to TF-IDF Vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "recipe_vectors = vectorizer.fit_transform(recipes)\n",
        "query_vector = vectorizer.transform([query])\n",
        "\n",
        "# Compute similarity\n",
        "similarities = cosine_similarity(query_vector, recipe_vectors)\n",
        "\n",
        "# Find the best match\n",
        "best_match_index = similarities.argmax()\n",
        "print(f\"Best Match (TF-IDF): {recipes[best_match_index]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaxXKC-hUl71",
        "outputId": "8eb83c49-544a-4dea-aec5-a917f8a20ed1"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Match (TF-IDF): Mild chicken stew with vegetables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3Ô∏è‚É£ Using Word2Vec Embeddings for Recipe Similarity\n",
        "üìå What happens?\n",
        "\n",
        "Instead of word counts, each word is represented by a pre-trained embedding vector\n",
        "Synonyms like \"hot\" and \"spicy\" will have similar representations"
      ],
      "metadata": {
        "id": "L_vQYA4bWMGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recipes = [\n",
        "    \"spicy chicken curry with rice\",\n",
        "    \"mild chicken stew with vegetables\",\n",
        "    \"hot and spicy beef stir fry\",\n",
        "    \"vegan lentil soup with coconut milk\",\n",
        "    \"grilled salmon with garlic butter\",\n",
        "    \"pasta with creamy tomato sauce\",\n",
        "    \"chocolate cake with vanilla frosting\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "OcnPRPdZXcP8"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üõ† Step 2: Tokenize Recipes\n",
        "Since Word2Vec expects tokenized sentences, split them into lists of words."
      ],
      "metadata": {
        "id": "aplRDDf7YRul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Tokenize recipes into words\n",
        "tokenized_recipes = [recipe.lower().split() for recipe in recipes]\n",
        "print(tokenized_recipes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMQW2nBkWPC8",
        "outputId": "8203a657-48a5-4bc9-cd0e-547fefce4c71"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['spicy', 'chicken', 'curry', 'with', 'rice'], ['mild', 'chicken', 'stew', 'with', 'vegetables'], ['hot', 'and', 'spicy', 'beef', 'stir', 'fry'], ['vegan', 'lentil', 'soup', 'with', 'coconut', 'milk'], ['grilled', 'salmon', 'with', 'garlic', 'butter'], ['pasta', 'with', 'creamy', 'tomato', 'sauce'], ['chocolate', 'cake', 'with', 'vanilla', 'frosting']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† Step 3: Train the Word2Vec Model\n",
        "Now, train the model on your recipe dataset."
      ],
      "metadata": {
        "id": "gLfkCN1yYV7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Word2Vec\n",
        "word2vec = Word2Vec(sentences=tokenized_recipes, vector_size=100, window=5, min_count=1, workers=4, epochs=50)\n",
        "\n",
        "# Save the model\n",
        "word2vec.save(\"app_word2vec.model\")\n"
      ],
      "metadata": {
        "id": "-t7S3w7lXfhh"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-lnd8ObYXt9",
        "outputId": "926d6c9c-1765-4478-c5a8-e044faac3a63"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app_word2vec.model  \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîç Step 4: Test Word Similarity\n",
        "Once trained, you can check similar words."
      ],
      "metadata": {
        "id": "rnXrAq13ZL3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "word2vec = Word2Vec.load(\"app_word2vec.model\")\n",
        "\n",
        "# Find words similar to 'spicy'\n",
        "print(word2vec.wv.most_similar(\"chocolate\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0jewb6qYakR",
        "outputId": "3f20e586-6e48-4fba-8296-96846e3d69aa"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('vegetables', 0.28531914949417114), ('spicy', 0.18995699286460876), ('vegan', 0.10841172933578491), ('with', 0.1029624417424202), ('stir', 0.09823568910360336), ('rice', 0.09716187417507172), ('salmon', 0.08844675868749619), ('cake', 0.06361950188875198), ('chicken', 0.05991499871015549), ('grilled', 0.04435737803578377)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ö° Step 5: Convert a Recipe to a Vector\n",
        "To compare recipe similarity, convert an entire recipe into a vector."
      ],
      "metadata": {
        "id": "GVFhUtZ4ZJ-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to get average word embeddings for a sentence\n",
        "def recipe_vector(sentence, model):\n",
        "    words = sentence.lower().split()\n",
        "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
        "\n",
        "# Example: Get vector for a new recipe\n",
        "new_recipe = \"spicy grilled chicken with garlic butter\"\n",
        "vector = recipe_vector(new_recipe, word2vec)\n",
        "print(vector)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxiAovNcYe6s",
        "outputId": "db515cb6-9299-464a-9d9a-0d3abae03ea1"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-3.88542801e-04  3.89766810e-03  2.91527179e-03  6.40447438e-03\n",
            " -1.75613665e-03  2.51298683e-04  1.07374333e-03  2.48682522e-03\n",
            " -2.95183086e-03 -1.73458271e-03  3.83635605e-04 -2.10796646e-03\n",
            "  3.89191764e-03  3.26596503e-03  1.77895778e-03  1.06846994e-04\n",
            "  3.98073765e-03  1.70906133e-03 -3.56898992e-03 -2.26425356e-03\n",
            "  1.95413406e-04  8.92759635e-05 -1.31530315e-03 -1.69266935e-03\n",
            "  3.62985977e-03  2.38684868e-03 -2.27567367e-03  2.62030284e-03\n",
            "  3.33555363e-04  2.77260318e-03  6.96676376e-04 -3.04600969e-03\n",
            "  6.48959074e-04 -4.84899944e-03 -1.69743737e-03  1.09213416e-03\n",
            "  2.24337936e-03 -2.47957348e-03 -3.96257819e-04  3.85728665e-04\n",
            " -5.33806102e-04  1.08142721e-03 -3.34651046e-03  6.85494859e-04\n",
            " -1.95841002e-03  5.61099849e-04 -2.70867051e-04 -1.30036846e-03\n",
            " -1.12414442e-03  9.16933583e-04  1.29569368e-03 -1.90785399e-03\n",
            " -6.40225224e-03  2.51590653e-04 -1.58424489e-04  1.02637417e-03\n",
            "  1.95680954e-03 -1.59009581e-03 -1.44484977e-03  8.26469157e-04\n",
            " -3.18185310e-03 -5.87507908e-04 -8.12591985e-04  7.28546511e-05\n",
            "  2.32939536e-04  4.86703496e-03  2.08370551e-03  7.24813901e-04\n",
            " -2.73674726e-03  3.44815128e-03 -3.19296960e-05  2.58791237e-03\n",
            "  1.73213240e-03 -2.66489596e-03 -1.63694331e-03  6.48751738e-04\n",
            "  1.46365946e-03  1.72483595e-03  7.76042521e-04  2.61529349e-05\n",
            " -4.95854719e-03 -7.31108710e-04  3.39152734e-03 -5.79863554e-04\n",
            " -1.02376228e-03 -4.44556121e-04  1.56235474e-03 -4.74499492e-03\n",
            "  1.65048370e-03  7.76967790e-05  1.49832061e-03 -5.46473020e-04\n",
            " -3.10619129e-04 -3.87372769e-04  2.57701060e-04 -1.10293960e-03\n",
            " -2.01080414e-03  1.23840373e-03 -1.17238524e-04  4.26453957e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî• Step 6: Find Similar Recipes in app\n",
        "Now, let's find the most similar recipe from app's database."
      ],
      "metadata": {
        "id": "N3Uq3KeWZpPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Convert all recipes into vectors\n",
        "recipe_vectors = np.array([recipe_vector(recipe, word2vec) for recipe in recipes])\n",
        "print('Recipe Vectors\\n')\n",
        "#print (recipe_vectors)\n",
        "\n",
        "# Convert the new query into a vector\n",
        "query_vector = recipe_vector(\"spicy grilled chicken with garlic butter\", word2vec).reshape(1, -1)\n",
        "print(query_vector)\n",
        "# Compute similarity scores\n",
        "similarities = cosine_similarity(query_vector, recipe_vectors)\n",
        "\n",
        "print('Similarities\\n')\n",
        "\n",
        "print(similarities)\n",
        "print('\\n')\n",
        "# Find the best match\n",
        "best_match_index = similarities.argmax()\n",
        "print(f\"Best Recipe Match: {recipes[best_match_index]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0xNMBy7ZmuR",
        "outputId": "65c45b0d-f36d-4b3e-f222-5021b7441603"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recipe Vectors\n",
            "\n",
            "[[-3.88542801e-04  3.89766810e-03  2.91527179e-03  6.40447438e-03\n",
            "  -1.75613665e-03  2.51298683e-04  1.07374333e-03  2.48682522e-03\n",
            "  -2.95183086e-03 -1.73458271e-03  3.83635605e-04 -2.10796646e-03\n",
            "   3.89191764e-03  3.26596503e-03  1.77895778e-03  1.06846994e-04\n",
            "   3.98073765e-03  1.70906133e-03 -3.56898992e-03 -2.26425356e-03\n",
            "   1.95413406e-04  8.92759635e-05 -1.31530315e-03 -1.69266935e-03\n",
            "   3.62985977e-03  2.38684868e-03 -2.27567367e-03  2.62030284e-03\n",
            "   3.33555363e-04  2.77260318e-03  6.96676376e-04 -3.04600969e-03\n",
            "   6.48959074e-04 -4.84899944e-03 -1.69743737e-03  1.09213416e-03\n",
            "   2.24337936e-03 -2.47957348e-03 -3.96257819e-04  3.85728665e-04\n",
            "  -5.33806102e-04  1.08142721e-03 -3.34651046e-03  6.85494859e-04\n",
            "  -1.95841002e-03  5.61099849e-04 -2.70867051e-04 -1.30036846e-03\n",
            "  -1.12414442e-03  9.16933583e-04  1.29569368e-03 -1.90785399e-03\n",
            "  -6.40225224e-03  2.51590653e-04 -1.58424489e-04  1.02637417e-03\n",
            "   1.95680954e-03 -1.59009581e-03 -1.44484977e-03  8.26469157e-04\n",
            "  -3.18185310e-03 -5.87507908e-04 -8.12591985e-04  7.28546511e-05\n",
            "   2.32939536e-04  4.86703496e-03  2.08370551e-03  7.24813901e-04\n",
            "  -2.73674726e-03  3.44815128e-03 -3.19296960e-05  2.58791237e-03\n",
            "   1.73213240e-03 -2.66489596e-03 -1.63694331e-03  6.48751738e-04\n",
            "   1.46365946e-03  1.72483595e-03  7.76042521e-04  2.61529349e-05\n",
            "  -4.95854719e-03 -7.31108710e-04  3.39152734e-03 -5.79863554e-04\n",
            "  -1.02376228e-03 -4.44556121e-04  1.56235474e-03 -4.74499492e-03\n",
            "   1.65048370e-03  7.76967790e-05  1.49832061e-03 -5.46473020e-04\n",
            "  -3.10619129e-04 -3.87372769e-04  2.57701060e-04 -1.10293960e-03\n",
            "  -2.01080414e-03  1.23840373e-03 -1.17238524e-04  4.26453957e-03]]\n",
            "Similarities\n",
            "\n",
            "[[0.39211792 0.39340618 0.31760937 0.22102378 0.7634536  0.14201415\n",
            "  0.28614128]]\n",
            "\n",
            "\n",
            "Best Recipe Match: grilled salmon with garlic butter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Sample dataset\n",
        "tokenzied_texts = [[\"spicy\", \"chicken\", \"curry\"], [\"hot\", \"beef\", \"stew\"]]\n",
        "\n",
        "# Train a small Word2Vec model\n",
        "word2vec = Word2Vec(sentences=tokenzied_texts, vector_size=100, min_count=1, window=5, workers=4, epochs=50)\n",
        "\n",
        "# Example: Get the vector for \"spicy\"\n",
        "print(word2vec.wv[\"spicy\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxFGK0YgZvBs",
        "outputId": "d2698e73-407b-44e6-be59-f193538dcbc7"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-8.7274825e-03  2.1301615e-03 -8.7354420e-04 -9.3190884e-03\n",
            " -9.4281426e-03 -1.4107180e-03  4.4324086e-03  3.7040710e-03\n",
            " -6.4986930e-03 -6.8730675e-03 -4.9994122e-03 -2.2868442e-03\n",
            " -7.2502876e-03 -9.6033178e-03 -2.7436293e-03 -8.3628409e-03\n",
            " -6.0388758e-03 -5.6709289e-03 -2.3441375e-03 -1.7069972e-03\n",
            " -8.9569986e-03 -7.3519943e-04  8.1525063e-03  7.6904297e-03\n",
            " -7.2061159e-03 -3.6668312e-03  3.1185520e-03 -9.5707225e-03\n",
            "  1.4764392e-03  6.5244664e-03  5.7464195e-03 -8.7630618e-03\n",
            " -4.5171441e-03 -8.1401607e-03  4.5956374e-05  9.2636338e-03\n",
            "  5.9733056e-03  5.0673080e-03  5.0610625e-03 -3.2429171e-03\n",
            "  9.5521836e-03 -7.3564244e-03 -7.2703874e-03 -2.2653891e-03\n",
            " -7.7856064e-04 -3.2161034e-03 -5.9258583e-04  7.4888230e-03\n",
            " -6.9751858e-04 -1.6249407e-03  2.7443992e-03 -8.3591007e-03\n",
            "  7.8558037e-03  8.5361041e-03 -9.5840869e-03  2.4462664e-03\n",
            "  9.9049713e-03 -7.6658037e-03 -6.9669187e-03 -7.7365171e-03\n",
            "  8.3959233e-03 -6.8133592e-04  9.1444086e-03 -8.1582209e-03\n",
            "  3.7430846e-03  2.6350426e-03  7.4271322e-04  2.3276759e-03\n",
            " -7.4690939e-03 -9.3583735e-03  2.3545765e-03  6.1484552e-03\n",
            "  7.9856887e-03  5.7358947e-03 -7.7733636e-04  8.3061643e-03\n",
            " -9.3363142e-03  3.4061326e-03  2.6675343e-04  3.8572443e-03\n",
            "  7.3857834e-03 -6.7251669e-03  5.5844807e-03 -9.5222248e-03\n",
            " -8.0445886e-04 -8.6887367e-03 -5.0986730e-03  9.2892265e-03\n",
            " -1.8582619e-03  2.9144264e-03  9.0712793e-03  8.9381328e-03\n",
            " -8.2084350e-03 -3.0123137e-03  9.8866057e-03  5.1044310e-03\n",
            " -1.5880871e-03 -8.6920215e-03  2.9615164e-03 -6.6758976e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to get average word embeddings for a sentence\n",
        "def recipe_vector(sentence, model):\n",
        "    words = sentence.lower().split()\n",
        "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
        "\n",
        "# Example: Get vector for a new recipe\n",
        "new_recipe = \"spicy grilled chicken with garlic butter\"\n",
        "vector = recipe_vector(new_recipe, word2vec)\n",
        "print(vector)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmzxBi7pey3E",
        "outputId": "ec2dc8f2-1e77-4cd1-d1b1-9656d022b558"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-7.93335121e-03  1.68572832e-03 -4.02526092e-03 -5.78183634e-03\n",
            " -2.85413582e-03  2.21132976e-03  2.81521725e-03  2.90354295e-03\n",
            " -5.30477986e-03  1.76225789e-04 -5.65334130e-03  1.18003122e-03\n",
            " -7.73494551e-03 -3.78333963e-03 -3.86010366e-03 -6.30535651e-03\n",
            " -4.57371678e-03 -7.53160566e-06  1.72686647e-03 -3.34095489e-03\n",
            " -4.09199018e-03 -4.61567007e-03  7.98116345e-03  8.47356394e-03\n",
            " -4.97396570e-03 -1.43339555e-03  1.93278119e-03 -2.04622792e-03\n",
            " -3.56501085e-03  3.55463196e-03  6.30816910e-03 -3.26597411e-03\n",
            " -1.69644202e-03 -8.73128977e-03  4.26398963e-03  1.49974902e-03\n",
            "  1.49053673e-03  4.28068731e-03  2.14433484e-03 -9.15726298e-04\n",
            "  5.66702848e-03 -7.09268451e-03 -8.49771220e-03  3.38777294e-03\n",
            "  2.70983553e-03 -5.06432028e-03  1.40555215e-03  3.84735456e-03\n",
            "  2.02801428e-03 -4.37246030e-03  3.38580227e-03 -2.00599339e-03\n",
            "  8.90639424e-03  2.03099567e-03 -5.48693212e-03 -2.43560341e-03\n",
            "  1.03608705e-04 -8.37306958e-03 -3.99500318e-03 -7.11993501e-03\n",
            "  6.62285555e-03 -3.42274318e-03  5.83198201e-03 -3.70940240e-03\n",
            "  1.75317749e-04  8.28197750e-04  5.36120124e-03  5.73688606e-03\n",
            " -5.96557185e-03 -1.37464143e-04 -1.64352206e-03  6.03973400e-03\n",
            "  2.44439533e-03  4.58395714e-03  1.11985393e-03  7.60345999e-03\n",
            " -5.85493119e-03  6.09067595e-03  3.92806577e-03 -2.84524192e-03\n",
            " -3.11289681e-04 -7.18170870e-03  4.25371435e-03 -6.15854189e-03\n",
            " -3.86695354e-03 -8.40868056e-03  1.60515239e-03  5.63957822e-03\n",
            " -5.59290778e-03 -9.39208316e-04  6.10389374e-03  2.11258978e-03\n",
            " -1.46353315e-03 -3.62307020e-03  6.26439741e-03 -1.47046940e-03\n",
            "  2.31111981e-03 -1.93658494e-03  1.87446875e-03 -1.83136808e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = [\n",
        "    \"A quick brown fox jumps over the lazy dog.\",\n",
        "    \"Birds of a feather flock together\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "CCqnJPNJfrJi"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sample_text = [\n",
        "    \"A quick brown fox jumps over the lazy dog.\", \"Machine Learning is Great\"\n",
        "]\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "\n",
        "# Fit & transform text\n",
        "count_vectors = count_vectorizer.fit_transform(sample_text)\n",
        "\n",
        "# Print feature names\n",
        "print(\"Feature Names:\", count_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Print count vector representation\n",
        "print(\"Count Vectorizer Representation:\\n\", count_vectors.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJ0No5RpsoK2",
        "outputId": "227541e2-2259-4d2e-8250-868b78fa4776"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names: ['brown' 'dog' 'fox' 'great' 'is' 'jumps' 'lazy' 'learning' 'machine'\n",
            " 'over' 'quick' 'the']\n",
            "Count Vectorizer Representation:\n",
            " [[1 1 1 0 0 1 1 0 0 1 1 1]\n",
            " [0 0 0 1 1 0 0 1 1 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit & transform text\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(sample_text)\n",
        "\n",
        "# Feature names (words)\n",
        "print(\"Feature Names:\", tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "\n",
        "# Convert to array\n",
        "print(\"TF-IDF Representation:\\n\", tfidf_vectors.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uGY8zjTsp2v",
        "outputId": "6f1b9725-adf5-49e0-cad0-86138e00c5d2"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names: ['brown' 'dog' 'fox' 'great' 'is' 'jumps' 'lazy' 'learning' 'machine'\n",
            " 'over' 'quick' 'the']\n",
            "TF-IDF Representation:\n",
            " [[0.35355339 0.35355339 0.35355339 0.         0.         0.35355339\n",
            "  0.35355339 0.         0.         0.35355339 0.35355339 0.35355339]\n",
            " [0.         0.         0.         0.5        0.5        0.\n",
            "  0.         0.5        0.5        0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Tokenize text\n",
        "tokenized_text = [sentence.lower().split() for sentence in sample_text]\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec = Word2Vec(sentences=tokenized_text, vector_size=15, min_count=1, window=5)\n",
        "\n",
        "# Get word vector for 'quick'\n",
        "print(\"Word2Vec Representation for 'quick':\\n\", word2vec.wv['quick'])\n",
        "\n",
        "# Average sentence embedding\n",
        "import numpy as np\n",
        "\n",
        "def sentence_embedding(sentence, model):\n",
        "    words = sentence.lower().split()\n",
        "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
        "\n",
        "# Compute sentence vectors\n",
        "word2vec_vectors = np.array([sentence_embedding(sentence, word2vec) for sentence in sample_text])\n",
        "print(\"Word2Vec Sentence Representation:\\n\", word2vec_vectors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xQQYcm3uAL6",
        "outputId": "f30c3ad2-1e85-4eef-c8fd-b226b460d8fa"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec Representation for 'quick':\n",
            " [ 0.03580226  0.05179676 -0.03844338  0.04955574  0.04416997 -0.024732\n",
            " -0.05830428  0.03624978  0.04339837 -0.00525033 -0.04473237 -0.0472395\n",
            " -0.01664707  0.03428836 -0.02443492]\n",
            "Word2Vec Sentence Representation:\n",
            " [[ 0.00252982  0.01001862  0.01330935 -0.00651709  0.01192626  0.01457651\n",
            "  -0.0199782  -0.00322744  0.01386847  0.00894945  0.00713015  0.01558428\n",
            "  -0.00046466 -0.01658536  0.01551047]\n",
            " [-0.01693707 -0.00912823  0.04208317 -0.00269584 -0.01975534 -0.01650045\n",
            "   0.04017171  0.0094373  -0.00563573  0.01055243 -0.01681479  0.01174436\n",
            "  -0.02385937 -0.01485522  0.00094813]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Tokenize text\n",
        "tokenized_text = [sentence.lower().split() for sentence in sample_text]\n",
        "\n",
        "print(tokenized_text)\n",
        "# Train Word2Vec model\n",
        "word2vec = Word2Vec(sentences=tokenized_text, vector_size=15, min_count=1, window=5, workers=4, epochs=50)\n",
        "\n",
        "# Cosine similarity function\n",
        "def cosine_sim(word1, word2, model):\n",
        "    vec1 = model.wv[word1].reshape(1, -1)\n",
        "    vec2 = model.wv[word2].reshape(1, -1)\n",
        "    return cosine_similarity(vec1, vec2)[0][0]\n",
        "\n",
        "# Find similarity\n",
        "similarity = cosine_sim(\"quick\", \"fox\", word2vec)\n",
        "print(f\"Cosine Similarity between 'quick' and 'fox': {similarity:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JL7O874buOjM",
        "outputId": "30dc2beb-3403-4f8a-90f9-11a5274bae02"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['a', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.'], ['machine', 'learning', 'is', 'great']]\n",
            "Cosine Similarity between 'quick' and 'fox': 0.1030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0nqO3WZguzZa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}